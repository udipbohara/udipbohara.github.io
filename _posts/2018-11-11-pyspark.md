---
layout: post
title:  "Analyzing Baseball data using PySpark"
info: "Mini-projects done leveraging Pyspark in Google Cloud Platform"
tech: "Pyspark, GCP"
img: "/assets/img/pyspark/pyspark.png" 
img_dimensions: ["300","200"]
concepts: "Spark, Cloud Computing, Natural Language Processing"
type: "blog"
tags: ["PySPark"]
---


TOC: 
- Simple SQL Join
- Aggregation with SQL Join
- Pyspark Streaming
- End-to-End Data Processing 


### Task 1: Simple SQL Join

Data website: https://github.com/chadwickbureau/baseballdatabank

steps: 
- get the data from the github to your working directory
- copy necessary files to your bucket

```terminal

$ curl -L https://github.com/chadwickbureau/baseballdatabacnk/archive/v2019/2/zip > data.zip
$ unzip data.zip
$ gsutil cp Batting.csv gs://bohara-udip/Batting.csv
```
<img src="/assets/img/pyspark/2a.png">


PySpark SQL script to find the major league baseball players who attended a college or university in Erie. The output should include their first and last names, the full name of the school they attended, each year they attended, and the city of Erie.


```python
#!/usr/bin/python
import pyspark

spark = (
    pyspark.sql.SparkSession
    .builder
    .getOrCreate()
)
sc = spark.sparkContext()
school = (
    spark.read.format("csv").option("header", "true").option("inferSchema", "true")
    .load("gs://bohara-udip/Schoolswh.csv")
    .select("schoolID", "name_full", "city")
)
school = (
    school.filter(school.city == "Erie")
)
college = (
    spark.read.format("csv").option("header", "true").option("inferSchema", "true")
    .load("gs://bohara-udip/CollegePlayingwh.csv")
    .select("schoolID", "playerID", "yearID")
)
result1 = (
    school.join(college, school.schoolID == college.schoolID)
    .drop("schoolID")
)
people = (
    spark.read.format("csv").option("header", "true").option("inferSchema", "true")
    .load("gs://bohara-udip/Peoplewh.csv")
    .select("playerID", "nameFirst", "nameLast")
)
result = (
    people.join(result1, result1.playerID == people.playerID)
    .drop("playerID")
    .select("nameFirst", "nameLast", "yearID", "name_full", "city")
    .show(12)
)

```
Output:

<img src="/assets/img/pyspark/2b.png" height='200'>


Multiple joins:

Here is an example of a multi-join to get the salaries of players (induced in Hall of Fame) based on their home runs.

```python
import pyspark

spark = (
 pyspark.sql.SparkSession
 .builder
 .getOrCreate()
)
sc=spark.sparkContext
salaries = (
  spark.read.format("csv").option("header","true").option("inferSchema","true")
  .load("gs://bohara-udip/Salaries.csv")
  .select("playerID","yearID","salary")
)
salaries=(
   salaries.filter(salaries.yearID == 2016)
   .drop("yearID")
)
names = (
  spark.read.format("csv").option("header","true").option("inferSchema","true")
  .load("gs://bohara-udip/PeopleWh.csv")
  .select("playerID","nameFirst","nameLast")
)
players = (
   names.join(salaries,names.playerID==salaries.playerID)
   .drop(salaries.playerID)
)
homeruns = (
   spark.read.format("csv").option("header","true").option("inferSchema","true")
  .load("gs://bohara-udip/BattingWh.csv")
  .groupBy("playerID")
  .agg(F.sum("HR").alias("careerHR"))
)
result=(
   players.join(homeruns,players.playerID==homeruns.playerID)
   .drop("playerID")
   .sort("careerHR",ascending=False)
   .show(100)
)
```

Output:

<img src="/assets/img/pyspark/2c.png" height='200'>
---

### Task 2 : Pyspark SQL with aggregation

PySpark script to generate a list of all of the players who have been voted into the Hall Of Fame. Each record should include the player's first and last names and the year he was voted in.

```python
#!/usr/bin/python
import pyspark

sc = pyspark.SparkContext()

header = ["playerID","yearID","votedBy","ballots","needed","votes","inducted","category","needed_note"]

players = (
    sc.textFile("gs://bohara-udip/HallOfFame.csv")
    .map(lambda s: s.encode("utf-8").split(","))
    .map(lambda a: dict(zip(header,a)))
    .filter(lambda d: d["inducted"]=="Y")
    .filter(lambda d: d["category"]=="Player")
    .sortBy(lambda d: d["yearID"])
    .map(lambda t:(t["playerID"],[t["yearID"]]))
)

peopleHeader = ["playerID","birthYear","birthMonth","birthDay","birthCountry","birthState","birthCity","deathYear","deathMonth","deathDay","deathCountry","deathState","deathCity","nameFirst","nameLast","nameGiven","weight","height","bats","throws","debut","finalGame","retroID","bbrefID"]

names = (
    sc.textFile("gs://bohara-udip/People.csv")
    .map(lambda s:s.encode("utf-8").split(","))
    .map(lambda a:dict(zip(peopleHeader,a)))
    .map(lambda t:(t["playerID"],(t["nameFirst"],t["nameLast"])))
)
result1 = (
    names.join(players)
)
battingHeader = ["playerID","yearID","stint","teamID","lgID","G","AB","R","H","2B","3B","HR","RBI","SB","CS","BB","SO","IBB","HBP","SH","SF","GIDP"]
hitters = (
       sc.textFile("gs://bohara-udip/Batting.csv")
       .map(lambda s:s.encode("utf-8").split(","))
       .map(lambda a:dict(zip(battingHeader,a)))
       .map(lambda d: (d["playerID"],int(d["HR"])))
       .groupByKey()
       .mapValues(sum)
        .map(lambda t:(t[0], [str(t[1])]))
)
result2=(
    result1.join(hitters)
    .sortBy(lambda d:(d[1][0][1]))
    .map(lambda t:",".join(t[1][0][0])+","+",".join(t[1][0][1])+","+",".join(t[1][1]))
    .collect()
)

print(result2)

```

<img src="/assets/img/pyspark/2.png">

----

### Task 3: Pyspark Data-Wrangling and Streaming


#### a) Simple data wrangling:

Getting data from a website (html) and cleaning it in a presentable form:

task - Get batting averages of Leonard Kyle Dykstra

```terminal
$ curl -L https://www.retrosheet.org/1990/Idyks10010071990.htm > dykstra
$ gsutil cp dykstra gs://bohara-udip/dykstra
```

The raw-data looks like this:
<img src="/assets/img/pyspark/3_a.png" >

```python

import pyspark
sc = pyspark.SparkContext()

days = (
   sc.textFile("gs://bohara-udip/dykstra")
   .map(lambda s:s.encode("utf-8"))
   .filter(lambda s:s[:2]=="<A") #first two characters to be <A
   .map(lambda s:s.split(">")) 
   .map(lambda a:[a[1][:10].replace(" ","0"),a[4].split()[22]]) #grab the date and replace empty date with 0 and grab the batting average
   .collect()
)
```
Output:

<img src="/assets/img/pyspark/3_a_2.png" > 


#### b) : Streaming the stocks data for the Coca-cola company using Pyspark Streaming and [Ubidots](https://ubidots.com)

The data is pretty straightforward, it is the CSV consisting of stocks for the company.

<img src="/assets/img/pyspark/3a.png">


```python
#!/usr/bin/python
from __future__ import print_function
import pyspark
import pyspark.streaming
from time import sleep
import requests
from datetime import datetime
sc = pyspark.SparkContext()
ssc = pyspark.streaming.StreamingContext(sc,1)
#-----------------------------------------------------------
coke = (
   sc.textFile("gs://bohara-udip/COKE.csv")
   .map(lambda s:s.encode("utf-8"))
   .collect()
)
rddQueue = []

#loop through each string and make it an RDD
for day in coke:
 rddQueue +=  [sc.parallelize([day])]

url = "http://things.ubidots.com/api/v1.6/devices/practice"
headers = {"X-Auth-Token": "A1E-DgVyyJgAY7VX1fLjwoMl8z8CPW0YXh", "Content-Type": "application/json"}

#send data to ubidots in a properly formatted way
def sendData(a):
   dt_array = a[0].split("/")
   month = int(dt_array[0])
   day = int(dt_array[1])
   year = int(dt_array[2])
   dt = datetime(year,month,day)
   ms=round((dt-datetime(1970,1,1)).total_seconds()*1000)
   ms = "%.0f" % ms
   requests.post(url=url,headers=headers,json={"ma":{"value":a[1],"timestamp":ms}})


#UDF (user defined function) to get the average price with the date
def f(obj):
   dt_array = map(lambda a:a[0],obj)
   dt = dt_array[-1]
   price_array = map(lambda a:a[1],obj)
   avgPrice = reduce(lambda x,y:x+y,price_array)/len(price_array)
   return([dt,avgPrice])

inputStream = ssc.queueStream(rddQueue)

#prep dat ato send
output=(
   inputStream
   .window(5,1)
   .map(lambda s:s.split(","))
   .map(lambda a:(1,[a[0],float(a[1])]))
   .groupByKey()
   .mapValues(f)
   .map(lambda t:t[1])
)

#send data
output.foreachRDD(lambda rdd: rdd.foreach(sendData))
output.foreachRDD(lambda rdd: print(rdd.collect()))
ssc.start()
sleep(150)
ssc.stop()
```

__Output:__
<img src="/assets/img/pyspark/3a2.png" height ='200'>



#### c) PySpark Streaming with Data Wrangling


PySpark script that will stream the start speeds for every pitch, with the correct timestamp, for a recent Phillies game and send the data to Ubidots.


Data : [raw_data_link](http://gd2.mlb.com/components/game/mlb/year_2019/month_04/day_02/gid_2019_04_02_phimlb_wasmlb_1/inning/inning_all.xml) 

Snippet of the data:
<img src="/assets/img/pyspark/3b.png" height ='200'>

```python
#!/usr/bin/python
from __future__ import print_function
import pyspark
import pyspark.streaming
from time import sleep
import requests
from datetime import datetime

sc = pyspark.SparkContext()
ssc = pyspark.streaming.StreamingContext(sc,1)

#wrangling to get the dates and the start_speed using filtering 
days = (
   sc.textFile("gs://bohara-udip/philly")
    .map(lambda s:s.encode("utf-8").strip())
    .filter(lambda s:"start_speed" in s)
    .map(lambda s:s.split('" '))
    .map(lambda a: a[28][10:28], a[23][13:17])
    .filter(lambda a:[23][13:17]!=" ")
    .collect()
)
rddQueue = []
#add it up to the queue 
for day in days:
   rddQueue += [sc.parallelize([day])]

url = "http://things.ubidots.com/api/v1.6/devices/practice"
headers = {"X-Auth-Token": "A1E-DgVyyJgAY7VX1fLjwoMl8z8CPW0YXh", "Content-Type": "application/json"}

#send data for streaming
def sendData(a):
   year = int(a[0][0:4])
   month = int(a[0][5:7])
   day = int(a[0][8:10])
   hour = int(a[0][11:13])
   minute = int(a[0][14:16])
   second = int(a[0][17])
   dt = datetime(year,month,day,hour,minute,second)
   ms=round((dt-datetime(1970,1,1)).total_seconds()*1000)
   ms = "%.0f" % ms
   requests.post(url=url,headers=headers,json={"philly":{"value":float(a[1]),"timestamp":ms}})

inputStream = ssc.queueStream(rddQueue)
output = (inputStream)
output.foreachRDD(lambda rdd: rdd.foreach(sendData))
output.foreachRDD(lambda rdd: print(rdd.collect()))

ssc.start()
sleep(10)
ssc.stop()
```
Outputs:

<div>
    <img src="/assets/img/pyspark/1a.png" width='200' style='float:left'/>
    <img src="/assets/img/pyspark/1b.png" width ='500'/>
</div>

---

### Task 4 : End-to-End Data Cleaning using PySpark

Generate csv file with one column has date of the every home game that the Phillies played then next to the date, how many home runs the Phillies hit.

```terminal
$ curl -L https://www.retrosheet.org/events/2017eve.zip > data.zip
$ unzip data.zip
$ cp 2017PHI.EVEN philliesv1
```
Complete description of the events file can be found [here](https://www.retrosheet.org/eventfile.htm)

The formatting of the data is very undersirable and needs processing:


Awk: text processing language and reads file line by line hence making us able to modify it. Awk by default identifies white space, we need to put commas. 

Using Awk to clean the data: 

```terminal
awk '
   BEGIN{FS=","}
   {sub(/\r/,"")}
   /^id/{x = $2}
   {print x","$0}
    ' 
    philliesv1 > philliesMod
```

__Before and After Awk cleanup__
<div>
<img src="/assets/img/pyspark/4.png" height='500' width='300' style='float:left'/>
<img src="/assets/img/pyspark/4a.png" height='500' width='450' /> 
</div>

PySpark can pass in UDF (User-Defined Functions) for wrangling the data and getting desired output 

```python
import pyspark
sc = pyspark.SparkContext()

#extract the date
def f(a):
  year = a[0][3:7]
  month = a[0][7:9]
  day = a[0][9:11]
  date = "{}-{}-{}".format(year,month,day)
  #conditions for a homerun that can be inferred in the event file above
  if(a[7][0] == "H" and a[7][1] != "P"):
      HR = 1
  else:
      HR = 0
  player = a[4] #grab the playername
  return((player+","+date,HR))


#every single string begins with 13 characters that represents ID and a ‘,’
#It should leave us with just plays

homeruns=(
           sc.textFile("gs://bohara-udip/philliesMod")
           .map(lambda s:s.encode("utf-8"))

           .filter(lambda s:s[13:17]=="play")
           .map(lambda s:s.split(","))
            #.filter(lambda a:a[4]=="mcgwm001" or a[4]=="sosas001") can be used to filter by player ID's
           .map(f)
           .groupByKey()
           .mapValues(sum)
           .sortBy(lambda t:t[0]) #sorting by date
           .map(lambda t:t[0]+","+str(t[1]))
           .coalesce(1)
           .saveAsTextFile("gs://bohara-udip/output")
)
```

Output:
<img src="/assets/img/pyspark/4b.png">

This can be put into any visual platform to get a desired result.
For example:
<br>
__Home runs for Phillies in 2017__
<img src="/assets/img/pyspark/4c.png">






