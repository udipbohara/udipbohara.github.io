---
layout: post
title:  "ArXiv Recommendation System"
info:  "Recommendation engine built with NLP and Graph Theory for ArXiv papers."
tags: ["Machine Learing", "Visualization", "NLP"]
tech : "python, Neo4j, transformers, pyspark, gephi, sigma.js"
img: "/assets/img/arXiv/transformer.png"
concepts: "Natural Language Processing, Graph Theory, Network Analysis "
type: "blog"
img_dimensions: ["650","400"]
#link: "https://github.com/udipbohara/info_extraction_receipts"
---

  <div style="text-align: center">
  <i class="fa fa-code"></i> <a href="https://github.com/udipbohara/arXiv-recommendation">Code to this Repo</a>
  </div>
<br>


__Note: This Project is Ongoing and will be updated accordingly.__

__Objective:__ Build recommendation system that is powered by graph theory and natural language processing for the papers in ArXiv under 'Computer Science'

----

At the time of the when this project was started, 1747307 papers were present in the (dataset)[https://www.kaggle.com/Cornell-University/arxiv/notebooks]. Of these papers, 'Artificial Intelligence', 'Machine Learning' and 'Computer Vision and Pattern Recognition' were selected corresponding to their respective tags/categories defined in ArXiv system. 61425 papers were extracted. 

Example of one paper in the dataset:

```python
({'id': '0704.0001',
  'submitter': 'Pavel Nadolsky',
  'authors': "C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan",
  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\n  LHC energies',
  'comments': '37 pages, 15 figures; published version',
  'journal-ref': 'Phys.Rev.D76:013009,2007',
  'doi': '10.1103/PhysRevD.76.013009',
  'report-no': 'ANL-HEP-PR-07-12',
  'categories': 'hep-ph',
  'license': None,
  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\npresented for the production of massive photon pairs at hadron colliders. All\nnext-to-leading order perturbative contributions from quark-antiquark,\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\nall-orders resummation of initial-state gluon radiation valid at\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\nspecified in which the calculation is most reliable. Good agreement is\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\nmore detailed tests with CDF and DO data. Predictions are shown for\ndistributions of diphoton pairs produced at the energy of the Large Hadron\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\nboson are contrasted with those produced from QCD processes at the LHC, showing\nthat enhanced sensitivity to the signal can be obtained with judicious\nselection of events.\n',
  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},
   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],
  'update_date': '2008-11-26',
  'authors_parsed': [['Balázs', 'C.', ''],
   ['Berger', 'E. L.', ''],
   ['Nadolsky', 'P. M.', ''],
   ['Yuan', 'C. -P.', '']]},
 {'id': '0704.0002',
  'submitter': 'Louis Theran',
  'authors': 'Ileana Streinu and Louis Theran',
  'title': 'Sparsity-certifying Graph Decompositions',
  'comments': 'To appear in Graphs and Combinatorics',
  'journal-ref': None,
  'doi': None,
  'report-no': None,
  'categories': 'math.CO cs.CG',
  'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',
  'abstract': '  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\nalgorithmic solutions to a family of problems concerning tree decompositions of\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\nreceived increased attention in recent years. In particular, our colored\npebbles generalize and strengthen the previous results of Lee and Streinu and\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\nalso present a new decomposition that certifies sparsity based on the\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\nWestermann and Hendrickson.\n',
  'versions': [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'},
   {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}],
  'update_date': '2008-12-13',
  'authors_parsed': [['Streinu', 'Ileana', ''], ['Theran', 'Louis', '']]})
```

```python
list_of_interest = ('cs.LG','cs.AI','cs.CV')
```



### Part 1: Topic Modelling:

LDA Topic Model:

LDA is a generative probabilistic model that assumes each topic is a mixture over an 
underlying set of words, and each document is a mixture of over a set of topic probabilities.
_Each document can be described by a distribution of topics and each topic can be described by a distribution of words_ [Source](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158)


<div>
    <center>
    <img src="/assets/img/arXiv/LDA_dirichlet.png" width="600"/>
        <br>
     <text><b> Fig : Illustration of LDA Allocation Method</b> <br> 
         <i> (Source: https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29)</i>
     </text>
    </center>
</div>

<br>

*"LDA is a three-level hierarchical Bayesian mLDAodel, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document"*-LDA authors [paper](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)

We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:
psi, the distribution of words for each topic K and
phi, the distribution of topics for each document i.

Parameters:<br>
_Alpha parameter_ is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document. <br>
_Beta parameter_ is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.


_Part-of-Speech tagging  and Noun-phrases extraction with spaCy:_

[spaCy](https://spacy.io) is a Python framework that can do many Natural Language Processing (NLP) tasks. Considering the first abstract of the corpus "Message passing algorithms have proved surprisingly successful in solving hard constraint satisfaction problems on sparse random graphs..........This conjecture is confirmed by numerical simulations." Let us take the example of the first sentence into consideration.
Since my topic modelling reasoning is to find similar papers (which would cover similar topics). I want to focus on Noun-phrases. This is because I am not particularly interested in the semantic meaning behind the abstract but rather on contextual similarities in them.
For the example, the noun phrases that are extracted are as follows : 'Message, algorithms, hard constraint satisfaction problems, sparse random graphs'

<p style="text-align: center;"><b>POS tagging with spaCy of Entire Sentence</b></p>
<img src="/assets/img/arXiv/pos_entire.png" height="120">

<br>

<p style="text-align: center;"><b>POS tagging of Extracted/Filtered Noun-Phrases</b></p>

<img src="/assets/img/arXiv/pos_noun.png">


_Adding Bigrams and Tri-grams:_

Adding Bigrams and Tri-grams are highly beneficial in gathering contextual adherence. Here are some of the bigrams that were found which were then grouped together while doing the modeling. 
```
(b'constraint', b'satisfaction')
(b'belief', b'propagation')
(b'propagation', b'bp')
(b'copula', b'entropy')
(b'aesthetically', b'pleasing')
(b'randomly', b'generated')
(b'partial', b'observability')
(b'partially', b'observable')
(b'observable', b'markov')
(b'markov', b'decision')
```

Using Bayes Rule, 
For our particular situation, the _generalized_ posterior (probability of getting a word based on a vector topic) is given by the following:


$$p(topic | word) = \frac{p(word | topic) p(topic)}{p(word)} =  \frac{p(word | topic) p(topic)}{\int_{topic}^{}p(word | topic)p(topic)dtopic} $$

Here, 
p(topic) is the prior, p(word | topic) is the likelihood and p(word) is the evidence. Here p(word) / the denominator is intractable because it has to evaluate the probability for all possible topic vectors; which causes dimensionality issues, whereas the numerator is for a single realization of a topic.


__Mallet__: 

The difference between Mallet and Gensim’s standard LDA is that Gensim uses a Variational Bayes sampling method ( approximate of the posterier distribution ) which is generally faster but less precise that Mallet’s Gibbs Sampling (Monte Carlo Markov Chain- iterative method to find the optimal posterier disribution- conditional probability  which iterates based on two main conditions: first part how much each document likes a topic and second how much each topic likes a word.


__Topic Coherence__ :

CV is based on four parts: (i) segmentation of the data into word pairs, (ii) calculation of word or word pair probabilities, (iii) calculation of a confirmation measure that quantifies how strongly a word set supports another word set, and finally (iv) aggregation of individual confirmation measures into an overall coherence score.

With the dataset, noun phrases with mallet lda (Gibbs sampling) with 20 topics yielded a better coherence score of 0.51 compared to 0.40 of gensim lda (Variational Bayes).


<img src="/assets/img/arXiv/mallet_results.png" height="200">

Here is the top 20 words for 5 topics from the best model. 
```
[(0,
  '0.046*"knowledge" + 0.027*"set" + 0.023*"question" + 0.021*"approach" + '
  '0.019*"event" + 0.019*"process" + 0.017*"paper" + 0.015*"concept" + '
  '0.014*"rule" + 0.013*"case" + 0.013*"work" + 0.013*"form" + 0.012*"problem" '
  '+ 0.012*"result" + 0.012*"type" + 0.011*"simple" + 0.011*"theory" + '
  '0.010*"explanation" + 0.009*"program" + 0.009*"general"'),
 (1,
  '0.140*"feature" + 0.074*"information" + 0.049*"level" + '
  '0.048*"representation" + 0.044*"attention" + 0.028*"local" + '
  '0.025*"temporal" + 0.023*"spatial" + 0.020*"art" + 0.018*"mechanism" + '
  '0.018*"global" + 0.016*"experiment" + 0.015*"propose" + 0.015*"module" + '
  '0.013*"context" + 0.012*"extensive" + 0.012*"multi" + 0.012*"semantic" + '
  '0.010*"fusion" + 0.010*"modality"'),
 (2,
  '0.043*"state" + 0.043*"policy" + 0.041*"agent" + 0.036*"learn" + '
  '0.034*"environment" + 0.034*"action" + 0.032*"dynamic" + '
  '0.024*"reinforcement" + 0.022*"control" + 0.022*"learning" + 0.018*"human" '
  '+ 0.015*"behavior" + 0.015*"reward" + 0.014*"goal" + 0.013*"game" + '
  '0.013*"simulation" + 0.012*"system" + 0.012*"rl" + 0.011*"robot" + '
  '0.011*"space"'),
 (3,
  '0.049*"user" + 0.026*"datum" + 0.018*"system" + 0.016*"decision" + '
  '0.015*"group" + 0.014*"individual" + 0.014*"online" + 0.013*"information" + '
  '0.012*"bias" + 0.012*"time" + 0.011*"recommendation" + 0.011*"item" + '
  '0.011*"privacy" + 0.010*"activity" + 0.009*"product" + 0.009*"paper" + '
  '0.009*"data" + 0.009*"social" + 0.008*"people" + 0.008*"study"'),
 (4,
  '0.040*"machine" + 0.037*"research" + 0.034*"technique" + '
  '0.033*"application" + 0.024*"learning" + 0.021*"field" + 0.019*"challenge" '
  '+ 0.018*"system" + 0.018*"analysis" + 0.015*"tool" + 0.014*"design" + '
  '0.013*"paper" + 0.013*"future" + 0.012*"area" + 0.011*"open" + '
  '0.011*"development" + 0.011*"current" + 0.010*"recent" + 0.010*"potential" '
  '+ 0.010*"ai"'),
 (5,
  '0.065*"system" + 0.053*"adversarial" + 0.034*"attack" + 0.028*"detection" + '
  '0.019*"robust" + 0.018*"signal" + 0.017*"robustness" + 0.017*"change" + '
  '0.016*"time" + 0.016*"input" + 0.016*"paper" + 0.015*"work" + 0.014*"base" '
  '+ 0.014*"scenario" + 0.013*"condition" + 0.013*"perturbation" + '
  '0.012*"result" + 0.012*"accuracy" + 0.012*"vehicle" + 0.011*"traffic"'),
```


Graph based systems are highly intuitive as they give a visual as well as spatial representation of a network.


### Part 2: Citation Network:
Semanticscholar API was used for generating citations and citations for 48767 papers were found 

387535 total citations were found for 48767 papaers. The most cited papers can clearly be seen from a network analysis point of view. The nodes are papers with connections depicted in the directed graph below. The nodes are colored/

The layout of the graph (paper:1)---[:cited/referenced]-->(paper:2)

<img src="/assets/img/arXiv/citation_network.png">

These networks can be highly informative not just visually but to analyze the most cited papers with respect to others. 

<!---
__Centrality Alogrithms__:
They’re useful because they identify the most important nodes and help us understand group dynamics such as credibility, accessi‐ bility, the speed at which things spread, and bridges between groups.
-->


### Part 3: Similarities Using Text from 'Abstract' for the papers

__Calculating Cosine Similarities Using Tf-Idf with minimum threshold 0.5 for entire corpus (PySpark)__


_Tf_: Term Frequency also known as TF measures the number of times a term (word) occurs in a document.

_Df_: Document Frequency is the number of documents in which the word is present.

_Tf-Idf_: Multiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.

From this, we can derive a vector consisting of values for each word in the corpus. From this we can calculate cosine between two documents which is essentially the angle between the vectors on a scale of 0 (cos90 for orthogonal vectors)(least similar) to 1(cos 0)(most similar).

6500 Similarities were extracted with a threshold of (0.5). 

Pyspark script was ran in Google Cloud Platform get the desired similarities:
```python
def clean_text(c):
  c = lower(c)
  c = regexp_replace(c, "^rt ", "")
  c = regexp_replace(c, "(https?\://)\S+", "")
  c = regexp_replace(c, "[^a-zA-Z0-9\\s]", "")
  return c

df = rdd.toDF(["abstract", "ID"])
clean_text_df = df.select('ID',clean_text(col("abstract")).alias("abstract"))
clean_text_df = clean_text_df.withColumn('abstract', trim(clean_text_df.abstract))orm(tf)
#tokenize words
tokenizer = Tokenizer(inputCol="abstract", outputCol="words")
stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')
# remove stop words
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
# define stage 3 :tf and idf
hashingTF = HashingTF(inputCol="words", outputCol="tf")
idf = IDF(inputCol="tf", outputCol="feature").fit(tf)
#compute L2 norm
normalizer = Normalizer(inputCol="feature", outputCol="norm")
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF,normalizer])
# fit the pipeline for the trainind data
model = pipeline.fit(clean_text_df)
# transform the data
data = model.transform(clean_text_df)
dot_udf = F.udf(lambda x,y: float(x.dot(y)) if float(x.dot(y)) > 0.5 else 0, DoubleType())

result2 = data.alias("i").join(data.alias("j"), F.col("i.ID") < F.col("j.ID")).select(
        F.col("i.ID").alias("i"), 
        F.col("j.ID").alias("j"), 
        dot_udf("i.norm", "j.norm").alias("dot"))\
    .sort("i", "j")\
    .na.drop(subset=["dot"])

result2.write.parquet('gs://output_spark/calculated_similarities.parquet')
```
### Result

Nodes represent Papers, Edges represent Similarities and are color coded according to the Similarity Scores. High Degree papers (papers with more similar papers) have larger size. 


<img src="/assets/img/arXiv/cosine0.5.png" width="50%" height="50%">


__Neo4j/Cyper code to form a Graph Database__

```sql
LOAD CSV WITH HEADERS FROM 'file:///similarities.csv' AS row
WITH toString(row[1]) AS paper1, toString(row[2]) AS paper2, toFloat(row[3]) AS similarity_value
MERGE (p1:Paper1{paper1_id: paper1})
  SET p1.paperName = paper1
MERGE (p2:Paper2{paper2_id: paper2})
  SET p2.paperName = paper2
MERGE (p1)-[rel:similar_to {similarity_value: similarity_value}]->(p2)
RETURN count(rel)
```

We can immediately catch duplicate papers:

```sql
MATCH (p1)-[s:similar_to]-(p2)
WHERE s.similarity_value =  1
RETURN p1,s,p2
```
<img src="/assets/img/arXiv/similarity1.png" width="300" height="300">


----

__Calculating Cosine Similarities Using Sentence trasnformers (DistilBert) in batches and returning top 100 for chunksize/batch size 1000 (Python)__

268661 Similarities


```python
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')
paraphrases_whole = util.paraphrase_mining(model, whole_corpus, top_k=100)
```

<img src="/assets/img/arXiv/transformer.png">


----
<!---

#### Part 2: Topic Modeling



```python
from pyspark.ml.clustering import LDA
num_topics = 6
max_iter = 10
lda = LDA(k=num_topics, 
          maxIter=max_iter, 
          featuresCol='tf_idf_features')
lda_model = lda.fit(tfidf_result)

vocab = tf_model.vocabulary
def get_words(token_list):
    return [vocab[token_id] for token_id in token_list]
udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))

num_top_words = 7
topics = lda_model
     .describeTopics(num_top_words)
     .withColumn('topicWords', udf_to_words(F.col('termIndices')))
topics.select('topic', 'topicWords').show(truncate=100)

```
-->


