---
layout: post
title:  "ArXiv Recommendation System"
info:  "Recommendation engine built with NLP and Graph Theory for ArXiv papers."
tags: ["Machine Learing", "Visualization", "NLP"]
tech : "python, Neo4j, transformers, pyspark, gephi, sigma.js"
img: "/assets/img/arXiv/transformer.png"
concepts: "Natural Language Processing, Graph Theory, Network Analysis "
type: "blog"
img_dimensions: ["650","400"]
#link: "https://github.com/udipbohara/info_extraction_receipts"
---



__Note: This Project is Ongoing and will be updated accordingly.__

__Objective:__ Build recommendation system that is powered by graph theory and natural language processing for the papers in ArXiv under 'Computer Science'

----

At the time of the when this project was started, 1747307 papers were present in the (dataset)[https://www.kaggle.com/Cornell-University/arxiv/notebooks]. Of these papers, 'Artificial Intelligence', 'Machine Learning' and 'Computer Vision and Pattern Recognition' were selected corresponding to their respective tags/categories defined in ArXiv system. 61425 papers were extracted. 

Example of one paper in the dataset:

```json
({'id': '0704.0001',
  'submitter': 'Pavel Nadolsky',
  'authors': "C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan",
  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\n  LHC energies',
  'comments': '37 pages, 15 figures; published version',
  'journal-ref': 'Phys.Rev.D76:013009,2007',
  'doi': '10.1103/PhysRevD.76.013009',
  'report-no': 'ANL-HEP-PR-07-12',
  'categories': 'hep-ph',
  'license': None,
  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\npresented for the production of massive photon pairs at hadron colliders. All\nnext-to-leading order perturbative contributions from quark-antiquark,\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\nall-orders resummation of initial-state gluon radiation valid at\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\nspecified in which the calculation is most reliable. Good agreement is\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\nmore detailed tests with CDF and DO data. Predictions are shown for\ndistributions of diphoton pairs produced at the energy of the Large Hadron\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\nboson are contrasted with those produced from QCD processes at the LHC, showing\nthat enhanced sensitivity to the signal can be obtained with judicious\nselection of events.\n',
  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},
   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],
  'update_date': '2008-11-26',
  'authors_parsed': [['Balázs', 'C.', ''],
   ['Berger', 'E. L.', ''],
   ['Nadolsky', 'P. M.', ''],
   ['Yuan', 'C. -P.', '']]},
 {'id': '0704.0002',
  'submitter': 'Louis Theran',
  'authors': 'Ileana Streinu and Louis Theran',
  'title': 'Sparsity-certifying Graph Decompositions',
  'comments': 'To appear in Graphs and Combinatorics',
  'journal-ref': None,
  'doi': None,
  'report-no': None,
  'categories': 'math.CO cs.CG',
  'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',
  'abstract': '  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\nalgorithmic solutions to a family of problems concerning tree decompositions of\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\nreceived increased attention in recent years. In particular, our colored\npebbles generalize and strengthen the previous results of Lee and Streinu and\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\nalso present a new decomposition that certifies sparsity based on the\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\nWestermann and Hendrickson.\n',
  'versions': [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'},
   {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26:00 GMT'}],
  'update_date': '2008-12-13',
  'authors_parsed': [['Streinu', 'Ileana', ''], ['Theran', 'Louis', '']]})
```

```python
list_of_interest = ('cs.LG','cs.AI','cs.CV')
```


Graph based systems are highly intuitive as they give a visual as well as spatial representation of a network.


### Citation Network:
Semanticscholar API was used for generating citations and citations for 48767 papers were found 

387535 total citations were found for 48767 papaers. The most cited papers can clearly be seen from a network analysis point of view. 


<img src="/assets/img/arXiv/citation_network.png">


<!---
__Centrality Alogrithms__:
They’re useful because they identify the most important nodes and help us understand group dynamics such as credibility, accessi‐ bility, the speed at which things spread, and bridges between groups.
-->
### Part 1: Similarities Using Text from 'Abstract' for the papers

__Calculating Cosine Similarities Using Tf-Idf with minimum threshold 0.5 for entire corpus (PySpark)__


_Tf_: Term Frequency also known as TF measures the number of times a term (word) occurs in a document.

_Df_: Document Frequency is the number of documents in which the word is present.

_Tf-Idf_: Multiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.

From this, we can derive a vector consisting of values for each word in the corpus. From this we can calculate cosine between two documents which is essentially the angle between the vectors on a scale of 0 (cos90 for orthogonal vectors)(least similar) to 1(cos 0)(most similar).

6500 Similarities were extracted with a threshold of (0.5). 

Pyspark script was ran in Google Cloud Platform get the desired similarities:
```python
def clean_text(c):
  c = lower(c)
  c = regexp_replace(c, "^rt ", "")
  c = regexp_replace(c, "(https?\://)\S+", "")
  c = regexp_replace(c, "[^a-zA-Z0-9\\s]", "")
  return c

df = rdd.toDF(["abstract", "ID"])
clean_text_df = df.select('ID',clean_text(col("abstract")).alias("abstract"))
clean_text_df = clean_text_df.withColumn('abstract', trim(clean_text_df.abstract))orm(tf)
#tokenize words
tokenizer = Tokenizer(inputCol="abstract", outputCol="words")
stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')
# remove stop words
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
# define stage 3 :tf and idf
hashingTF = HashingTF(inputCol="words", outputCol="tf")
idf = IDF(inputCol="tf", outputCol="feature").fit(tf)
#compute L2 norm
normalizer = Normalizer(inputCol="feature", outputCol="norm")
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF,normalizer])
# fit the pipeline for the trainind data
model = pipeline.fit(clean_text_df)
# transform the data
data = model.transform(clean_text_df)
dot_udf = F.udf(lambda x,y: float(x.dot(y)) if float(x.dot(y)) > 0.5 else 0, DoubleType())

result2 = data.alias("i").join(data.alias("j"), F.col("i.ID") < F.col("j.ID")).select(
        F.col("i.ID").alias("i"), 
        F.col("j.ID").alias("j"), 
        dot_udf("i.norm", "j.norm").alias("dot"))\
    .sort("i", "j")\
    .na.drop(subset=["dot"])

result2.write.parquet('gs://output_spark/calculated_similarities.parquet')
```
### Result

Nodes represent Papers, Edges represent Similarities and are color coded according to the Similarity Scores. High Degree papers (papers with more similar papers) have larger size. 


<img src="/assets/img/arXiv/cosine0.5.png">


__Neo4j/Cyper code to form a Graph Database__

```sql
LOAD CSV WITH HEADERS FROM 'file:///similarities.csv' AS row
WITH toString(row[1]) AS paper1, toString(row[2]) AS paper2, toFloat(row[3]) AS similarity_value
MERGE (p1:Paper1{paper1_id: paper1})
  SET p1.paperName = paper1
MERGE (p2:Paper2{paper2_id: paper2})
  SET p2.paperName = paper2
MERGE (p1)-[rel:similar_to {similarity_value: similarity_value}]->(p2)
RETURN count(rel)
```

We can immediately catch duplicate papers:

```sql
MATCH (p1)-[s:similar_to]-(p2)
WHERE s.similarity_value =  1
RETURN p1,s,p2
```
<img src="/assets/img/arXiv/similarity1.png" width="300" height="300">


----

__Calculating Cosine Similarities Using Sentence trasnformers (DistilBert) in batches and returning top 100 for chunksize/batch size 1000 (Python)__

268661 Similarities


```python
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')
paraphrases_whole = util.paraphrase_mining(model, whole_corpus, top_k=100)
```

<img src="/assets/img/arXiv/transformer.png">


----
<!---

#### Part 2: Topic Modeling



```python
from pyspark.ml.clustering import LDA
num_topics = 6
max_iter = 10
lda = LDA(k=num_topics, 
          maxIter=max_iter, 
          featuresCol='tf_idf_features')
lda_model = lda.fit(tfidf_result)

vocab = tf_model.vocabulary
def get_words(token_list):
    return [vocab[token_id] for token_id in token_list]
udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))

num_top_words = 7
topics = lda_model
     .describeTopics(num_top_words)
     .withColumn('topicWords', udf_to_words(F.col('termIndices')))
topics.select('topic', 'topicWords').show(truncate=100)

```
-->