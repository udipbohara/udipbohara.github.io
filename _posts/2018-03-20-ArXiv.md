---
layout: post
title:  "ArXiv Recommendation System"
info:  "A robust and scalable method of network analysis using ArXiv dataset."
tags: ["Machine Learing", "Visualization", "NLP"]
tech : "python, Neo4j, transformers, pyspark, gephi, sigma.js"
img: "/assets/img/arXiv/transformer.png"
concepts: "Natural Language Processing, Graph Theory, Network Analysis "
type: "blog"
img_dimensions: ["650","400"]
#link: "https://github.com/udipbohara/info_extraction_receipts"
---



__Note: This Project is Ongoing and will be updated accordingly.__

__Objective:__ Build recommendation system that is powered by graph theory and natural language processing for the papers in ArXiv under 'Computer Science'

----
Graph based systems are highly intuitive as they give a visual as well as spatial representation of a network.




<!---
__Centrality Alogrithms__:
They’re useful because they identify the most important nodes and help us understand group dynamics such as credibility, accessi‐ bility, the speed at which things spread, and bridges between groups.
-->
### Part 1: Similarities Using Text from 'Abstract' for the papers

__Calculating Cosine Similarities Using Tf-Idf with minimum threshold 0.5 for entire corpus (PySpark)__

Nodes represent Papers, Edges represent Similarities and are color coded according to the Similarity Scores. High Degree papers (papers with more similar papers) have larger size. 

6500 Similarities

Pyspark Script Snippet: 
```python
def clean_text(c):
  c = lower(c)
  c = regexp_replace(c, "^rt ", "")
  c = regexp_replace(c, "(https?\://)\S+", "")
  c = regexp_replace(c, "[^a-zA-Z0-9\\s]", "")
  return c

df = rdd.toDF(["abstract", "ID"])
clean_text_df = df.select('ID',clean_text(col("abstract")).alias("abstract"))
clean_text_df = clean_text_df.withColumn('abstract', trim(clean_text_df.abstract))orm(tf)
#tokenize words
tokenizer = Tokenizer(inputCol="abstract", outputCol="words")
stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')
# remove stop words
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
# define stage 3 :tf and idf
hashingTF = HashingTF(inputCol="words", outputCol="tf")
idf = IDF(inputCol="tf", outputCol="feature").fit(tf)
#compute L2 norm
normalizer = Normalizer(inputCol="feature", outputCol="norm")
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF,normalizer])
# fit the pipeline for the trainind data
model = pipeline.fit(clean_text_df)
# transform the data
data = model.transform(clean_text_df)
dot_udf = F.udf(lambda x,y: float(x.dot(y)) if float(x.dot(y)) > 0.5 else 0, DoubleType())

result2 = data.alias("i").join(data.alias("j"), F.col("i.ID") < F.col("j.ID")).select(
        F.col("i.ID").alias("i"), 
        F.col("j.ID").alias("j"), 
        dot_udf("i.norm", "j.norm").alias("dot"))\
    .sort("i", "j")\
    .na.drop(subset=["dot"])

result2.write.parquet('gs://output_spark/calculated_similarities.parquet')
```
### Result

<img src="/assets/img/arXiv/cosine0.5.png">


__Neo4j/Cyper code to form a Graph Database__

```sql
LOAD CSV WITH HEADERS FROM 'file:///similarities.csv' AS row
WITH toString(row[1]) AS paper1, toString(row[2]) AS paper2, toFloat(row[3]) AS similarity_value
MERGE (p1:Paper1{paper1_id: paper1})
  SET p1.paperName = paper1
MERGE (p2:Paper2{paper2_id: paper2})
  SET p2.paperName = paper2
MERGE (p1)-[rel:similar_to {similarity_value: similarity_value}]->(p2)
RETURN count(rel)
```

We can immediately catch duplicate papers:

```sql
MATCH (p1)-[s:similar_to]-(p2)
WHERE s.similarity_value =  1
RETURN p1,s,p2
```
<img src="/assets/img/arXiv/similarity1.png" width="300" height="300">


----

__Calculating Cosine Similarities Using Sentence trasnformers (DistilBert) in batches and returning top 100 for chunksize/batch size 1000 (Python)__

268661 Similarities


```python
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')
paraphrases_whole = util.paraphrase_mining(model, whole_corpus, top_k=100)
```

<img src="/assets/img/arXiv/transformer.png">


----
<!---

#### Part 2: Topic Modeling



```python
from pyspark.ml.clustering import LDA
num_topics = 6
max_iter = 10
lda = LDA(k=num_topics, 
          maxIter=max_iter, 
          featuresCol='tf_idf_features')
lda_model = lda.fit(tfidf_result)

vocab = tf_model.vocabulary
def get_words(token_list):
    return [vocab[token_id] for token_id in token_list]
udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))

num_top_words = 7
topics = lda_model
     .describeTopics(num_top_words)
     .withColumn('topicWords', udf_to_words(F.col('termIndices')))
topics.select('topic', 'topicWords').show(truncate=100)

```
-->