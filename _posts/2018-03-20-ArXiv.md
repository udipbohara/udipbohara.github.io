---
layout: post
title:  "ArXiv Recommendation System"
info:  "A robust and scalable method of network analysis using ArXiv dataset."
tags: ["Machine Learing", "Visualization", "NLP"]
tech : "python, Neo4j, transformers, pyspark, gephi, sigma.js"
img: "/assets/img/arXiv/transformer.png"
concepts: "Natural Language Processing, Graph Theory, Network Analysis "
type: "blog"
img_dimensions: ["650","400"]
#link: "https://github.com/udipbohara/info_extraction_receipts"
---



__Note: This Project is Ongoing and will be updated accordingly.__

__Objective:__ Build recommendation system that is powered by graph theory and natural language processing for the papers in ArXiv under 'Computer Science'

----

### Part 1: Similarities Using Text from 'Abstract' for the papers

__Calculating Cosine Similarities Using Tf-Idf with minimum threshold 0.5 for entire corpus (PySpark)__

Nodes represent Papers, Edges represent Similarities and are color coded according to the Similarity Scores. High Degree papers (papers with more similar papers) have larger size. 

6500 Similarities

Pyspark Script Snippet: 
```python
def clean_text(c):
  c = lower(c)
  c = regexp_replace(c, "^rt ", "")
  c = regexp_replace(c, "(https?\://)\S+", "")
  c = regexp_replace(c, "[^a-zA-Z0-9\\s]", "")
  return c

df = rdd.toDF(["abstract", "ID"])
clean_text_df = df.select('ID',clean_text(col("abstract")).alias("abstract"))
clean_text_df = clean_text_df.withColumn('abstract', trim(clean_text_df.abstract))orm(tf)
#tokenize words
tokenizer = Tokenizer(inputCol="abstract", outputCol="words")
stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')
# remove stop words
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
# define stage 3 :tf and idf
hashingTF = HashingTF(inputCol="words", outputCol="tf")
idf = IDF(inputCol="tf", outputCol="feature").fit(tf)
#compute L2 norm
normalizer = Normalizer(inputCol="feature", outputCol="norm")
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF,normalizer])
# fit the pipeline for the trainind data
model = pipeline.fit(clean_text_df)
# transform the data
data = model.transform(clean_text_df)
dot_udf = F.udf(lambda x,y: float(x.dot(y)) if float(x.dot(y)) > 0.5 else 0, DoubleType())

result2 = data.alias("i").join(data.alias("j"), F.col("i.ID") < F.col("j.ID")).select(
        F.col("i.ID").alias("i"), 
        F.col("j.ID").alias("j"), 
        dot_udf("i.norm", "j.norm").alias("dot"))\
    .sort("i", "j")\
    .na.drop(subset=["dot"])

result2.write.parquet('gs://output_spark/calculated_similarities.parquet')
```
### Result

<img src="/assets/img/arXiv/cosine0.5.png">

----

__Calculating Cosine Similarities Using Sentence trasnformers (DistilBert) in batches and returning top 100 for chunksize/batch size 1000 (Python)__

268661 Similarities


```python
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')
paraphrases_whole = util.paraphrase_mining(model, whole_corpus, top_k=100)
```

<img src="/assets/img/arXiv/transformer.png">


----
<!---

#### Part 2: Topic Modeling



```python
from pyspark.ml.clustering import LDA
num_topics = 6
max_iter = 10
lda = LDA(k=num_topics, 
          maxIter=max_iter, 
          featuresCol='tf_idf_features')
lda_model = lda.fit(tfidf_result)

vocab = tf_model.vocabulary
def get_words(token_list):
    return [vocab[token_id] for token_id in token_list]
udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))

num_top_words = 7
topics = lda_model
     .describeTopics(num_top_words)
     .withColumn('topicWords', udf_to_words(F.col('termIndices')))
topics.select('topic', 'topicWords').show(truncate=100)

```
-->